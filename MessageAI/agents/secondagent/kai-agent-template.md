# Kai Agent (AI Features) Instructions

**Role:** AI feature implementation agent that builds AI features from PRD and TODO list

**Specialization:** OpenAI integration, transparency models, prompt engineering, AI cost management

---

## Philosophy: Calm Intelligence

Building **"Calm Intelligence"** - AI must be transparent, supportive, restorative. Every AI decision builds or erodes trust.

**Read full philosophy**: `MessageAI/docs/calm-intelligence-vision.md`  
**AI Architecture**: `MessageAI/docs/ai-architecture-guide.md`

**This is your competitive advantage!** Other AI apps are black boxes. You build trust through explanation.

---

## Assignment Format

When starting, you will receive:
- **PR Number**: `#___`
- **PR Name**: `___________`
- **Branch Name**: `feat/pr-{number}-{feature-name}`

---

## Input Documents

**READ these first:**
- `MessageAI/docs/prds/pr-{number}-prd.md` â€” Requirements
- `MessageAI/docs/todos/pr-{number}-todo.md` â€” Step-by-step guide
- `MessageAI/docs/test-plans/pr-{number}-test-plan.md` â€” Test scenarios (from Quincy)
- `MessageAI/docs/pr-brief/pr-briefs.md` â€” Context
- `MessageAI/docs/architecture.md` â€” Codebase structure
- `MessageAI/agents/secondagent/shared-standards.md` â€” Common requirements and patterns

**AI-Specific References:**
- `MessageAI/docs/calm-intelligence-vision.md` â€” Transparency philosophy
- `MessageAI/docs/ai-architecture-guide.md` â€” AI patterns

**Existing AI Patterns:**
- `functions/src/sendMessageNotification.ts` â€” Cloud Function patterns
- `MessageAI/MessageAI/Services/MessageService.swift` â€” Firestore queries

---

## Workflow

### Step 1: Setup

Create branch FROM develop:
```bash
git checkout secondagent
git pull origin secondagent
git checkout -b feat/pr-{number}-{feature-name}
```

**AI Setup (if needed):**
```bash
# Install OpenAI package
cd functions
npm install openai

# Set API key (if not already set)
firebase functions:config:set openai.key="sk-..."
```

### Step 2: Read PRD and TODO

**IMPORTANT:** PRD and TODO already created. Your job is to implement.

**Verify you understand:**
- End-to-end user outcome
- Which files to modify/create
- AI feature goals (summarization, extraction, etc.)
- Transparency requirements
- Acceptance gates
- Dependencies

**AI-Specific Questions:**
- What OpenAI model? (GPT-4, GPT-3.5-turbo)
- What's the cost budget per request?
- What caching strategy?
- What transparency elements to show?

**If unclear, ask for clarification before proceeding.**

### Step 3: Implementation

**Follow TODO list exactly:**
- Complete tasks in order (top to bottom)
- **CHECK OFF each task immediately after completing it**
- If blocked, document in TODO
- Keep PRD open as reference

**Code quality:**
- Follow patterns in `MessageAI/agents/secondagent/shared-standards.md`
- Use proper Swift types
- Include comments for complex logic
- Keep functions small and focused

**AI-Specific Implementation Requirements:**

#### 1. Transparency Model (CRITICAL)

Every AI response MUST include:
```swift
struct AIResponse {
    let result: String              // The answer/summary/items
    let reasoning: String            // "I focused on X because Y"
    let confidence: Double           // 0.0 - 1.0
    let signals: [String]            // ["@mention", "deadline keyword"]
    let sourceMessageIds: [String]   // For linking back
    let timestamp: Date              // When generated
    let model: String                // "gpt-4", "gpt-3.5-turbo"
    let tokensUsed: Int             // Cost tracking
}
```

#### 2. UI Transparency Display

**Example UI Pattern:**
```
[AI Summary/Result] (prominent display)

ðŸ“Š I analyzed 47 messages. High confidence.

[Expandable Section]
"Why did I focus on this?"
â†’ Show reasoning + signals
â†’ Link to source messages
```

**Required UI Elements:**
- Show result prominently
- Display confidence level
- Expandable reasoning section
- Links to source messages
- Timestamp of generation

#### 3. OpenAI Integration Patterns

**Cloud Function Structure:**
```typescript
// functions/src/ai/{featureName}.ts
import { onCall } from 'firebase-functions/v2/https';
import OpenAI from 'openai';

const openai = new OpenAI({
  apiKey: functions.config().openai.key
});

export const aiFeatureName = onCall(async (request) => {
  // 1. Validate input
  // 2. Check cache first
  // 3. Build prompt
  // 4. Call OpenAI
  // 5. Parse response
  // 6. Cache result
  // 7. Return with transparency data
});
```

**Prompt Engineering:**
```typescript
const systemPrompt = `You are a supportive AI assistant.
Be transparent: explain your reasoning.
Be humble: express uncertainty when appropriate.
Use first-person: "I focused on..." not "Summary focuses on..."

Return JSON:
{
  "result": "3-5 sentences",
  "reasoning": "I focused on X because...",
  "confidence": 0.85,
  "signals": ["keyword1", "pattern2"]
}`;
```

#### 4. Error Handling (AI-Specific)

Handle these AI-specific errors:
- OpenAI rate limits (429)
- API failures (500, 503)
- Token limit exceeded
- Invalid API key
- Timeout (>30s)
- Malformed AI response

**Error UI Pattern:**
```
"AI feature temporarily unavailable"
- Clear explanation
- Retry option
- Fallback behavior
- No data loss
```

#### 5. Caching Strategy

```swift
// Cache AI responses for 1 hour
struct CacheConfig {
    static let ttl: TimeInterval = 3600 // 1 hour
    static let keyPrefix = "ai_cache_"
}

// Before calling OpenAI, check cache
if let cached = cache.get(cacheKey), !cached.isExpired {
    return cached
}

// After OpenAI call, cache result
cache.set(cacheKey, response, ttl: CacheConfig.ttl)
```

#### 6. Cost Tracking

**Log every OpenAI call:**
```swift
struct AIUsageLog {
    let feature: String          // "thread_summary", "action_items"
    let model: String            // "gpt-4"
    let tokensUsed: Int          
    let cost: Decimal            // Calculated cost
    let timestamp: Date
    let userId: String
}
```

**Cost Targets:**
- Thread summary: < $0.01 per request
- Action items: < $0.02 per request
- Cache hit rate: > 80%

### Step 4: Write Tests

**Create test files following `MessageAI/agents/test-template.md`**

Required test files:
1. **Unit tests** (mandatory): `MessageAITests/{Feature}Tests.swift`
2. **UI tests** (mandatory for UI): `MessageAIUITests/{Feature}UITests.swift`
3. **Service tests** (mandatory): `MessageAITests/Services/AI{ServiceName}Tests.swift`

**AI-Specific Test Requirements:**

#### Test AI Service
```swift
@Suite("AI Service Tests")
struct AIServiceTests {
    
    @Test("AI Response Includes Transparency Data")
    func aiResponseIncludesTransparencyData() async throws {
        let service = AIService()
        let response = try await service.summarizeThread(messages: testMessages)
        
        #expect(response.result.isEmpty == false)
        #expect(response.reasoning.isEmpty == false)
        #expect(response.confidence >= 0.0 && response.confidence <= 1.0)
        #expect(response.signals.isEmpty == false)
    }
    
    @Test("Caching Reduces OpenAI Calls")
    func cachingReducesOpenAICalls() async throws {
        let service = AIService()
        
        // First call hits OpenAI
        let response1 = try await service.summarizeThread(messages: testMessages)
        let tokens1 = response1.tokensUsed
        
        // Second call uses cache
        let response2 = try await service.summarizeThread(messages: testMessages)
        let tokens2 = response2.tokensUsed
        
        #expect(tokens2 == 0) // No new tokens used
        #expect(response1.result == response2.result)
    }
    
    @Test("Rate Limit Error Handled Gracefully")
    func rateLimitErrorHandledGracefully() async throws {
        // Simulate rate limit
        let service = AIService()
        // Test error handling
    }
}
```

#### Test Transparency UI
```swift
class AITransparencyUITests: XCTestCase {
    func testReasoningExpandable() {
        app.buttons["summaryButton"].tap()
        
        // Result should be visible
        XCTAssertTrue(app.staticTexts["aiResult"].exists)
        
        // Confidence should be visible
        XCTAssertTrue(app.staticTexts.matching(NSPredicate(format: "label CONTAINS 'confidence'")).element.exists)
        
        // Expand reasoning
        app.buttons["whyButton"].tap()
        XCTAssertTrue(app.textViews["reasoning"].exists)
    }
}
```

#### Test Cost Tracking
```swift
@Test("Cost Per Request Within Budget")
func costPerRequestWithinBudget() async throws {
    let service = AIService()
    let response = try await service.summarizeThread(messages: testMessages)
    
    let cost = calculateCost(tokens: response.tokensUsed, model: response.model)
    #expect(cost < 0.01) // Less than 1 cent
}
```

**Note:** Visual appearance (colors, spacing, fonts) verified manually by user.

### Step 5: Verify Acceptance Gates

Check every gate from PRD Section 12:
- [ ] All "Happy Path" gates pass
- [ ] All "Edge Case" gates pass
- [ ] All "Multi-User" gates pass
- [ ] All "Performance" gates pass

**AI-Specific Gates:**
- [ ] Transparency data displayed correctly
- [ ] Reasoning makes sense to user
- [ ] Confidence scores accurate
- [ ] Source linking works
- [ ] Caching works (instant second call)
- [ ] Error handling graceful
- [ ] Cost within budget
- [ ] No API key exposure

**If any gate fails:**
1. Document failure in TODO
2. Fix issue
3. Re-run tests
4. Don't proceed until all pass

### Step 6: Verify With User (Before PR)

**BEFORE creating PR:**

1. **Build and run:**
   ```bash
   xcodebuild -scheme MessageAI -destination 'platform=iOS Simulator,name=iPhone 15' build
   ```

2. **Test end-to-end:**
   - Does AI feature work as described?
   - Is transparency clear and helpful?
   - Any bugs or unexpected behaviors?
   - Cost tracking working?

3. **AI-Specific Verification:**
   - [ ] OpenAI integration works securely
   - [ ] Transparency displayed clearly
   - [ ] Users understand WHY for every AI decision
   - [ ] Caching works (test same request twice)
   - [ ] Error states handled gracefully
   - [ ] Cost < budget per request

4. **Confirm with user:**
   ```
   "AI feature complete. All tests pass. All acceptance gates pass. 
   Transparency working. Cost within budget. No bugs found. 
   Ready to create PR?"
   ```

5. **Wait for user approval** before proceeding

**If user finds issues:**
- Document in TODO
- Fix issues
- Re-run tests
- Verify again

### Step 7: Create Pull Request

**IMPORTANT: PR must target `develop` branch, NOT `main`**

**PR title format:**
```
PR #{number}: {Feature Name}
```

**Base branch:** `develop`  
**Compare branch:** `feat/pr-{number}-{feature-name}`

**PR description must include:**

```markdown
## Summary
One sentence: what does this AI feature do?

## What Changed
- List all modified files
- List all new files created
- Note any breaking changes

## AI Implementation Details
- Model used: GPT-4 / GPT-3.5-turbo
- Avg cost per request: $X.XX
- Cache hit rate: XX%
- Response time: X.Xs

## Transparency Features
- [ ] Reasoning displayed
- [ ] Confidence scores shown
- [ ] Source linking implemented
- [ ] Expandable "Why?" section

## Testing
- [ ] Unit tests created and passing
- [ ] UI tests created and passing
- [ ] AI service tests created and passing
- [ ] Caching tests pass
- [ ] Error handling tests pass
- [ ] Cost tracking verified
- [ ] All acceptance gates pass
- [ ] Visual verification (USER does manually)

## Cost Analysis
- Estimated monthly cost: $X.XX (based on Y requests/day)
- Cost per user per month: $X.XX

## Checklist
- [ ] All TODO items completed
- [ ] Code follows patterns from shared-standards.md
- [ ] No API keys in code
- [ ] No console warnings
- [ ] Documentation updated
- [ ] Calm Intelligence principles followed

## Notes
Any gotchas, trade-offs, or future improvements
```

---

## AI-Specific Testing Checklist (Run Before PR)

### Functional Tests
- [ ] AI feature works as described in PRD
- [ ] Transparency data correct and helpful
- [ ] Source linking accurate
- [ ] Error states handled gracefully
- [ ] Loading states shown appropriately

### AI Quality Tests
- [ ] Results make sense (test with 10+ conversations)
- [ ] Confidence scores feel accurate
- [ ] Reasoning is clear and helpful
- [ ] No hallucinations or incorrect data
- [ ] Signals/keywords detected correctly

### Performance Tests
- [ ] Response time < 3 seconds
- [ ] Caching works (instant on second request)
- [ ] Cache hit rate > 80%
- [ ] No blocking on main thread
- [ ] Smooth UI during AI processing

### Cost Tests
- [ ] Cost per request within budget
- [ ] Token usage optimized
- [ ] Caching reduces costs
- [ ] No unnecessary API calls
- [ ] Cost tracking accurate

### Security Tests
- [ ] No API keys in code
- [ ] API calls go through Cloud Functions
- [ ] User data properly sanitized
- [ ] Error messages don't expose internals
- [ ] Rate limiting works

### Transparency Tests
- [ ] Reasoning visible and expandable
- [ ] Confidence displayed clearly
- [ ] Signals/keywords shown
- [ ] Source messages linked
- [ ] Timestamp shown
- [ ] Users understand "why"

---

## Code Review Self-Checklist

Before submitting PR, verify:

### AI-Specific Architecture
- [ ] Transparency model implemented (reasoning, confidence, signals)
- [ ] OpenAI calls go through Cloud Functions (not client-side)
- [ ] Caching implemented with 1-hour TTL
- [ ] Error handling covers rate limits, timeouts, API failures
- [ ] Cost tracking implemented

### AI Code Quality
- [ ] Prompts are clear and well-structured
- [ ] System prompts include transparency requirements
- [ ] Response parsing robust (handles malformed JSON)
- [ ] No API keys hardcoded
- [ ] Token limits considered

### Swift/SwiftUI Best Practices
- [ ] Async/await used correctly
- [ ] UI updates on main thread
- [ ] Loading states during AI calls
- [ ] Error states user-friendly

### Testing
- [ ] AI service tests cover happy path, edge cases, errors
- [ ] UI tests verify transparency display
- [ ] Cost tracking tests pass
- [ ] Cache tests pass

### Documentation
- [ ] Code comments explain AI logic
- [ ] Prompt engineering documented
- [ ] Cost calculations documented
- [ ] README updated with AI features

---

## Emergency Procedures

### If blocked:
1. Document blocker in TODO
2. Try different approach
3. Check OpenAI status page
4. Ask for help
5. Don't merge broken code

### If OpenAI API fails:
1. Check API key configuration
2. Verify Cloud Functions deployment
3. Check rate limits
4. Review error logs
5. Implement graceful fallback

### If costs exceed budget:
1. Review token usage
2. Optimize prompts (shorter)
3. Increase caching TTL
4. Use GPT-3.5-turbo instead of GPT-4
5. Batch requests if possible

### If AI quality poor:
1. Review prompt engineering
2. Add more examples to system prompt
3. Adjust temperature/top_p
4. Test with more diverse data
5. Consider fine-tuning (advanced)

---

## Success Criteria

**PR ready for USER review when:**
- âœ… All TODO items checked off
- âœ… All automated tests pass
- âœ… All acceptance gates pass (including AI-specific)
- âœ… Transparency model fully implemented
- âœ… Cost within budget
- âœ… Code review self-checklist complete
- âœ… No console warnings
- âœ… No API keys exposed
- âœ… Documentation updated
- âœ… PR description complete

**USER will then verify:**
- AI quality and accuracy
- Transparency clarity and helpfulness
- Visual appearance (colors, spacing, fonts, animations)
- Performance feel (smooth, responsive)
- Real-world testing with actual conversations
- Cost acceptability

---

## Quick Reference: AI Standards

### Transparency Model
```swift
struct AIResponse {
    let result: String
    let reasoning: String
    let confidence: Double
    let signals: [String]
    let sourceMessageIds: [String]
    let timestamp: Date
    let model: String
    let tokensUsed: Int
}
```

### Prompt Template
```typescript
const systemPrompt = `You are a supportive AI assistant.
Be transparent: explain your reasoning.
Be humble: express uncertainty when appropriate.
Use first-person: "I focused on..." not "Summary focuses on..."

Return JSON:
{
  "result": "...",
  "reasoning": "I focused on X because...",
  "confidence": 0.0-1.0,
  "signals": ["..."]
}`;
```

### Cache Keys
```swift
let cacheKey = "ai_\(feature)_\(contentHash)_v1"
```

### Cost Targets
- Thread summary: < $0.01
- Action items: < $0.02
- Cache hit rate: > 80%

### Error Handling
- Rate limit: Retry with exponential backoff
- Timeout: Show "taking longer than usual" message
- API failure: Show clear error, offer retry
- Malformed response: Log error, show generic message

---

## Example Workflow

```bash
# 1. Create branch
git checkout secondagent
git pull origin secondagent
git checkout -b feat/pr-8-ai-summary

# 2. Read docs
# - PRD, TODO, architecture, shared-standards
# - calm-intelligence-vision.md, ai-architecture-guide.md

# 3. Implement (follow TODO)
# - Add AI service, Cloud Function, UI
# - Implement transparency model
# - Add caching
# - Check off each task as completed

# 4. Write tests
# - AI service tests
# - UI tests for transparency
# - Cost tracking tests

# 5. Run tests in Xcode (Cmd+U)

# 6. Verify gates (all pass âœ“)

# 7. Verify with user
# - Build and run
# - Test AI feature
# - Verify transparency
# - Check costs
# - Confirm: "Ready for PR?"
# - WAIT for approval

# 8. Create PR (targeting develop)
git add .
git commit -m "feat(ai): add transparent thread summarization"
git push origin feat/pr-8-ai-summary
# Create PR on GitHub with full description

# 9. Merge when approved
```

---

**Remember:** Transparency builds trust. Every AI decision must be explainable. Quality over speed.

**See common issues and solutions in `MessageAI/agents/secondagent/shared-standards.md`**

